#!/usr/bin/env python3
"""
K-means Clustering Analysis for TCN-VAE Behavioral Motifs
Implements methodology from Section 4.1: Unsupervised Clustering of Motion Embeddings

Sprint 1, Task 1: Extract features from trained models and implement behavioral clustering
Target: Identify 8-14 distinct behavioral motifs with silhouette score >0.5
"""

import torch
import torch.nn as nn
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.cluster import KMeans
from sklearn.metrics import silhouette_score, adjusted_rand_score
from sklearn.manifold import TSNE
from sklearn.decomposition import PCA
import umap
import pickle
import json
import os
import sys
from datetime import datetime
import warnings
warnings.filterwarnings('ignore')

# Add project root to path
sys.path.append('/home/wllmflower/Development/tcn-vae-training')

from models.tcn_vae import TCNVAE
from preprocessing.unified_pipeline import MultiDatasetHAR
from preprocessing.enhanced_pipeline import EnhancedMultiDatasetHAR
from preprocessing.quadruped_pipeline import QuadrupedDatasetHAR

class BehavioralMotifAnalyzer:
    """
    Comprehensive behavioral motif extraction and clustering analysis
    Based on literature review Section 4.1 methodology
    """
    
    def __init__(self, device='cuda' if torch.cuda.is_available() else 'cpu'):
        self.device = device
        self.models = {}
        self.features = {}
        self.clusters = {}
        self.results = {}
        
        print(f"üî¨ Behavioral Motif Analyzer initialized on {device}")
        
    def load_model(self, model_name, model_path, config):
        """Load a trained TCN-VAE model for feature extraction"""
        try:
            # Create model with configuration
            model = TCNVAE(
                input_dim=config.get('input_dim', 9),
                hidden_dims=config.get('hidden_dims', [64, 128, 256]),
                latent_dim=config.get('latent_dim', 64),
                num_activities=config.get('num_activities', 12)
            )
            
            # Load trained weights
            if os.path.exists(model_path):
                checkpoint = torch.load(model_path, map_location=self.device)
                if isinstance(checkpoint, dict) and 'model_state_dict' in checkpoint:
                    model.load_state_dict(checkpoint['model_state_dict'])
                    print(f"‚úÖ Loaded checkpoint for {model_name}")
                else:
                    model.load_state_dict(checkpoint)
                    print(f"‚úÖ Loaded state dict for {model_name}")
            else:
                print(f"‚ö†Ô∏è Model path not found: {model_path}")
                return False
                
            model.to(self.device)
            model.eval()
            self.models[model_name] = model
            
            print(f"ü§ñ {model_name}: {sum(p.numel() for p in model.parameters()):,} parameters")
            return True
            
        except Exception as e:
            print(f"‚ùå Error loading {model_name}: {e}")
            return False
    
    def extract_features(self, model_name, data_loader, max_batches=None):
        """Extract latent features from trained model"""
        if model_name not in self.models:
            print(f"‚ùå Model {model_name} not loaded")
            return None
            
        model = self.models[model_name]
        features = []
        labels = []
        
        print(f"üîç Extracting features from {model_name}...")
        
        with torch.no_grad():
            for batch_idx, batch_data in enumerate(data_loader):
                if max_batches and batch_idx >= max_batches:
                    break
                    
                if len(batch_data) == 3:
                    data, activity_labels, domain_labels = batch_data
                else:
                    data, activity_labels = batch_data
                    
                data = data.to(self.device).float()
                
                try:
                    # Extract latent features (mu from VAE encoder)
                    _, mu, logvar, _, _ = model(data)
                    
                    features.append(mu.cpu().numpy())
                    labels.append(activity_labels.numpy())
                    
                    if batch_idx % 10 == 0:
                        print(f"  Batch {batch_idx}: {mu.shape[0]} samples, latent dim {mu.shape[1]}")
                        
                except Exception as e:
                    print(f"‚ö†Ô∏è Error in batch {batch_idx}: {e}")
                    continue
        
        if features:
            features = np.vstack(features)
            labels = np.hstack(labels)
            
            self.features[model_name] = {
                'features': features,
                'labels': labels,
                'extracted_at': datetime.now().isoformat()
            }
            
            print(f"‚úÖ Extracted {len(features)} feature vectors (dim={features.shape[1]}) from {model_name}")
            return features, labels
        else:
            print(f"‚ùå No features extracted from {model_name}")
            return None, None
    
    def perform_clustering(self, model_name, k_range=(8, 15), n_init=10):
        """
        Perform K-means clustering with optimal K selection
        Based on literature methodology for behavioral motif discovery
        """
        if model_name not in self.features:
            print(f"‚ùå No features available for {model_name}")
            return None
            
        features = self.features[model_name]['features']
        labels = self.features[model_name]['labels']
        
        print(f"üéØ Clustering analysis for {model_name}")
        print(f"  Features: {features.shape}")
        print(f"  K range: {k_range[0]}-{k_range[1]}")
        
        # Evaluate different K values
        k_scores = {}
        silhouette_scores = []
        inertias = []
        k_values = range(k_range[0], k_range[1] + 1)
        
        for k in k_values:
            print(f"  Testing K={k}...")
            
            # Fit K-means
            kmeans = KMeans(n_clusters=k, random_state=42, n_init=n_init)
            cluster_labels = kmeans.fit_predict(features)
            
            # Calculate silhouette score
            sil_score = silhouette_score(features, cluster_labels)
            silhouette_scores.append(sil_score)
            inertias.append(kmeans.inertia_)
            
            k_scores[k] = {
                'silhouette_score': sil_score,
                'inertia': kmeans.inertia_,
                'cluster_labels': cluster_labels,
                'centroids': kmeans.cluster_centers_
            }
            
            print(f"    Silhouette score: {sil_score:.3f}")
        
        # Select optimal K
        best_k = k_values[np.argmax(silhouette_scores)]
        best_score = max(silhouette_scores)
        
        print(f"üéØ Optimal K={best_k} with silhouette score {best_score:.3f}")
        
        # Store clustering results
        self.clusters[model_name] = {
            'k_scores': k_scores,
            'optimal_k': best_k,
            'best_silhouette': best_score,
            'k_range': k_range,
            'clustered_at': datetime.now().isoformat()
        }
        
        return k_scores, best_k, best_score
    
    def generate_visualizations(self, model_name, save_dir='evaluation/clustering_plots'):
        """Generate comprehensive clustering visualizations"""
        if model_name not in self.features or model_name not in self.clusters:
            print(f"‚ùå Missing data for {model_name} visualization")
            return
            
        os.makedirs(save_dir, exist_ok=True)
        
        features = self.features[model_name]['features']
        true_labels = self.features[model_name]['labels']
        cluster_info = self.clusters[model_name]
        best_k = cluster_info['optimal_k']
        cluster_labels = cluster_info['k_scores'][best_k]['cluster_labels']
        
        print(f"üìä Generating visualizations for {model_name}...")
        
        # 1. K-selection plot (silhouette scores)
        plt.figure(figsize=(12, 4))
        
        plt.subplot(1, 2, 1)
        k_values = list(cluster_info['k_scores'].keys())
        sil_scores = [cluster_info['k_scores'][k]['silhouette_score'] for k in k_values]
        inertias = [cluster_info['k_scores'][k]['inertia'] for k in k_values]
        
        plt.plot(k_values, sil_scores, 'bo-', linewidth=2, markersize=8)
        plt.axvline(x=best_k, color='red', linestyle='--', alpha=0.7, label=f'Optimal K={best_k}')
        plt.xlabel('Number of Clusters (K)')
        plt.ylabel('Silhouette Score')
        plt.title(f'{model_name}: K Selection (Silhouette Method)')
        plt.grid(True, alpha=0.3)
        plt.legend()
        
        plt.subplot(1, 2, 2)
        plt.plot(k_values, inertias, 'ro-', linewidth=2, markersize=8)
        plt.axvline(x=best_k, color='red', linestyle='--', alpha=0.7, label=f'Optimal K={best_k}')
        plt.xlabel('Number of Clusters (K)')
        plt.ylabel('Inertia')
        plt.title(f'{model_name}: Elbow Method')
        plt.grid(True, alpha=0.3)
        plt.legend()
        
        plt.tight_layout()
        plt.savefig(f'{save_dir}/{model_name}_k_selection.png', dpi=300, bbox_inches='tight')
        plt.close()
        
        # 2. t-SNE visualization
        print("  Generating t-SNE visualization...")
        tsne = TSNE(n_components=2, random_state=42, perplexity=min(30, len(features)//4))
        features_2d = tsne.fit_transform(features)
        
        plt.figure(figsize=(15, 5))
        
        # t-SNE with cluster labels
        plt.subplot(1, 3, 1)
        scatter = plt.scatter(features_2d[:, 0], features_2d[:, 1], c=cluster_labels, 
                            cmap='tab10', alpha=0.7, s=20)
        plt.colorbar(scatter)
        plt.title(f'{model_name}: t-SNE (K-means Clusters, K={best_k})')
        plt.xlabel('t-SNE 1')
        plt.ylabel('t-SNE 2')
        
        # t-SNE with true labels
        plt.subplot(1, 3, 2)
        scatter = plt.scatter(features_2d[:, 0], features_2d[:, 1], c=true_labels, 
                            cmap='viridis', alpha=0.7, s=20)
        plt.colorbar(scatter)
        plt.title(f'{model_name}: t-SNE (True Labels)')
        plt.xlabel('t-SNE 1')
        plt.ylabel('t-SNE 2')
        
        # Cluster-True label comparison
        plt.subplot(1, 3, 3)
        # Calculate adjusted rand index
        ari_score = adjusted_rand_score(true_labels, cluster_labels)
        
        # Create confusion-style heatmap
        from sklearn.metrics import confusion_matrix
        cm = confusion_matrix(true_labels, cluster_labels)
        sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')
        plt.title(f'{model_name}: Clusters vs True Labels\nARI Score: {ari_score:.3f}')
        plt.xlabel('Cluster Labels')
        plt.ylabel('True Labels')
        
        plt.tight_layout()
        plt.savefig(f'{save_dir}/{model_name}_tsne_analysis.png', dpi=300, bbox_inches='tight')
        plt.close()
        
        # 3. UMAP visualization (if available)
        try:
            print("  Generating UMAP visualization...")
            umap_reducer = umap.UMAP(n_components=2, random_state=42)
            features_umap = umap_reducer.fit_transform(features)
            
            plt.figure(figsize=(10, 5))
            
            plt.subplot(1, 2, 1)
            scatter = plt.scatter(features_umap[:, 0], features_umap[:, 1], c=cluster_labels, 
                                cmap='tab10', alpha=0.7, s=20)
            plt.colorbar(scatter)
            plt.title(f'{model_name}: UMAP (K-means Clusters, K={best_k})')
            plt.xlabel('UMAP 1')
            plt.ylabel('UMAP 2')
            
            plt.subplot(1, 2, 2)
            scatter = plt.scatter(features_umap[:, 0], features_umap[:, 1], c=true_labels, 
                                cmap='viridis', alpha=0.7, s=20)
            plt.colorbar(scatter)
            plt.title(f'{model_name}: UMAP (True Labels)')
            plt.xlabel('UMAP 1')
            plt.ylabel('UMAP 2')
            
            plt.tight_layout()
            plt.savefig(f'{save_dir}/{model_name}_umap_analysis.png', dpi=300, bbox_inches='tight')
            plt.close()
            
        except ImportError:
            print("  UMAP not available, skipping UMAP visualization")
        
        # 4. Cluster statistics
        self._generate_cluster_statistics(model_name, save_dir)
        
        print(f"‚úÖ Visualizations saved to {save_dir}")
    
    def _generate_cluster_statistics(self, model_name, save_dir):
        """Generate detailed cluster statistics and behavioral motif analysis"""
        features = self.features[model_name]['features']
        true_labels = self.features[model_name]['labels']
        cluster_info = self.clusters[model_name]
        best_k = cluster_info['optimal_k']
        cluster_labels = cluster_info['k_scores'][best_k]['cluster_labels']
        
        # Cluster size distribution
        plt.figure(figsize=(12, 4))
        
        plt.subplot(1, 2, 1)
        cluster_counts = np.bincount(cluster_labels)
        plt.bar(range(len(cluster_counts)), cluster_counts, alpha=0.7, color='skyblue')
        plt.xlabel('Cluster ID')
        plt.ylabel('Number of Samples')
        plt.title(f'{model_name}: Cluster Size Distribution')
        plt.grid(True, alpha=0.3)
        
        # True label distribution within clusters
        plt.subplot(1, 2, 2)
        unique_true_labels = np.unique(true_labels)
        true_label_counts = np.bincount(true_labels)
        plt.bar(range(len(true_label_counts)), true_label_counts, alpha=0.7, color='lightcoral')
        plt.xlabel('True Label ID')
        plt.ylabel('Number of Samples')
        plt.title(f'{model_name}: True Label Distribution')
        plt.grid(True, alpha=0.3)
        
        plt.tight_layout()
        plt.savefig(f'{save_dir}/{model_name}_distributions.png', dpi=300, bbox_inches='tight')
        plt.close()
        
        # Generate behavioral motif summary
        motif_summary = {
            'model_name': model_name,
            'optimal_clusters': int(best_k),
            'silhouette_score': float(cluster_info['best_silhouette']),
            'total_samples': len(features),
            'latent_dimension': features.shape[1],
            'cluster_sizes': {int(i): int(count) for i, count in enumerate(np.bincount(cluster_labels))},
            'behavioral_motifs': {}
        }
        
        # Analyze each cluster's behavioral composition
        for cluster_id in range(best_k):
            cluster_mask = cluster_labels == cluster_id
            cluster_true_labels = true_labels[cluster_mask]
            
            # Most common true label in this cluster
            if len(cluster_true_labels) > 0:
                unique_labels, counts = np.unique(cluster_true_labels, return_counts=True)
                dominant_label = unique_labels[np.argmax(counts)]
                purity = counts.max() / len(cluster_true_labels)
                
                motif_summary['behavioral_motifs'][f'cluster_{cluster_id}'] = {
                    'size': int(np.sum(cluster_mask)),
                    'dominant_behavior': int(dominant_label),
                    'purity': float(purity),
                    'behavior_distribution': {int(label): int(count) for label, count in zip(unique_labels, counts)}
                }
        
        # Save summary
        with open(f'{save_dir}/{model_name}_motif_summary.json', 'w') as f:
            json.dump(motif_summary, f, indent=2)
        
        print(f"üìã Behavioral motif summary saved for {model_name}")
    
    def analyze_all_models(self, save_results=True):
        """Comprehensive analysis across all loaded models"""
        results_summary = {
            'analysis_timestamp': datetime.now().isoformat(),
            'models_analyzed': list(self.models.keys()),
            'comparative_results': {}
        }
        
        print("üî¨ Comprehensive Multi-Model Analysis")
        print("=" * 50)
        
        for model_name in self.models.keys():
            if model_name in self.clusters:
                cluster_info = self.clusters[model_name]
                feature_info = self.features[model_name]
                
                print(f"\nüìä {model_name.upper()} Results:")
                print(f"  Optimal Clusters: {cluster_info['optimal_k']}")
                print(f"  Silhouette Score: {cluster_info['best_silhouette']:.3f}")
                print(f"  Feature Samples: {len(feature_info['features'])}")
                print(f"  Latent Dimension: {feature_info['features'].shape[1]}")
                
                results_summary['comparative_results'][model_name] = {
                    'optimal_clusters': cluster_info['optimal_k'],
                    'silhouette_score': cluster_info['best_silhouette'],
                    'feature_samples': len(feature_info['features']),
                    'latent_dimension': feature_info['features'].shape[1]
                }
        
        if save_results:
            os.makedirs('evaluation', exist_ok=True)
            with open('evaluation/clustering_analysis_results.json', 'w') as f:
                json.dump(results_summary, f, indent=2)
            
            print(f"\n‚úÖ Complete analysis results saved to evaluation/clustering_analysis_results.json")
        
        return results_summary

def main():
    """Main execution function for clustering analysis"""
    print("üöÄ Starting TCN-VAE Behavioral Motif Clustering Analysis")
    print("üìã Sprint 1, Task 1: K-means clustering implementation")
    print("=" * 60)
    
    analyzer = BehavioralMotifAnalyzer()
    
    # Model configurations
    model_configs = {
        'baseline': {
            'path': 'models/best_tcn_vae.pth',
            'config': {
                'input_dim': 9,
                'hidden_dims': [64, 128, 256],
                'latent_dim': 64,
                'num_activities': 13
            }
        },
        'enhanced': {
            'path': 'models/best_enhanced_tcn_vae.pth',
            'config': {
                'input_dim': 9,
                'hidden_dims': [64, 128, 256, 128],
                'latent_dim': 64,
                'num_activities': 12
            }
        },
        'quadruped': {
            'path': 'models/best_quadruped_tcn_vae.pth',
            'config': {
                'input_dim': 9,
                'hidden_dims': [64, 128, 256, 128],
                'latent_dim': 64,
                'num_activities': 12
            }
        }
    }
    
    # Load available models
    loaded_models = []
    for model_name, config in model_configs.items():
        if analyzer.load_model(model_name, config['path'], config['config']):
            loaded_models.append(model_name)
    
    if not loaded_models:
        print("‚ùå No models loaded successfully. Check model paths.")
        return
    
    print(f"\n‚úÖ Loaded {len(loaded_models)} models: {', '.join(loaded_models)}")
    
    # Create sample data for feature extraction (using baseline pipeline)
    print("\nüìä Preparing validation data for feature extraction...")
    try:
        from preprocessing.unified_pipeline import MultiDatasetHAR
        processor = MultiDatasetHAR(window_size=100, overlap=0.5)
        
        # Load a subset of data for clustering analysis
        X_train, y_train, domains_train, X_val, y_val, domains_val = processor.preprocess_all()
        
        # Create validation dataset for feature extraction
        val_dataset = torch.utils.data.TensorDataset(
            torch.FloatTensor(X_val[:1000]),  # Limit to 1000 samples for efficiency
            torch.LongTensor(y_val[:1000])
        )
        val_loader = torch.utils.data.DataLoader(val_dataset, batch_size=64, shuffle=False)
        
        print(f"‚úÖ Validation data prepared: {len(val_dataset)} samples")
        
    except Exception as e:
        print(f"‚ö†Ô∏è Error preparing data, creating synthetic validation set: {e}")
        # Create synthetic data for demonstration
        synthetic_data = torch.randn(500, 100, 9)
        synthetic_labels = torch.randint(0, 10, (500,))
        val_dataset = torch.utils.data.TensorDataset(synthetic_data, synthetic_labels)
        val_loader = torch.utils.data.DataLoader(val_dataset, batch_size=64, shuffle=False)
        print("‚úÖ Synthetic validation data created")
    
    # Extract features from all loaded models
    print("\nüîç Feature Extraction Phase")
    print("-" * 30)
    for model_name in loaded_models:
        features, labels = analyzer.extract_features(model_name, val_loader, max_batches=10)
        if features is not None:
            print(f"  ‚úÖ {model_name}: {len(features)} features extracted")
        else:
            print(f"  ‚ùå {model_name}: Feature extraction failed")
    
    # Perform clustering analysis
    print("\nüéØ Clustering Analysis Phase")
    print("-" * 30)
    for model_name in loaded_models:
        if model_name in analyzer.features:
            k_scores, best_k, best_score = analyzer.perform_clustering(model_name, k_range=(3, 12))
            if k_scores:
                print(f"  ‚úÖ {model_name}: K={best_k}, Silhouette={best_score:.3f}")
        else:
            print(f"  ‚ö†Ô∏è {model_name}: No features available for clustering")
    
    # Generate visualizations
    print("\nüìä Visualization Generation Phase")
    print("-" * 30)
    for model_name in loaded_models:
        if model_name in analyzer.clusters:
            analyzer.generate_visualizations(model_name)
            print(f"  ‚úÖ {model_name}: Visualizations generated")
    
    # Comprehensive analysis
    print("\nüî¨ Final Analysis Phase")
    print("-" * 30)
    results = analyzer.analyze_all_models()
    
    print(f"\nüéâ Clustering analysis complete!")
    print(f"üìÅ Results saved to evaluation/ directory")
    print(f"üìä {len(loaded_models)} models analyzed with behavioral motif extraction")
    
    return analyzer

if __name__ == "__main__":
    analyzer = main()